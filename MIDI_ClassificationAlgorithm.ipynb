{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MIDI_ClassificationAlgorithm.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]}},"cells":[{"metadata":{"id":"sX95jFWhJfdm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import pickle\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9ELi_UscQ1QU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def getBatch(data, labels, batchSize, iteration):\n","    startOfBatch = (iteration * batchSize) % len(data)\n","    endOfBacth = (iteration * batchSize + batchSize) % len(data)\n","\n","    if startOfBatch < endOfBacth:\n","        return data[startOfBatch:endOfBacth], labels[startOfBatch:endOfBacth]\n","    else:\n","        dataBatch = np.vstack((data[startOfBatch:],data[:endOfBacth]))\n","        labelsBatch = np.vstack((labels[startOfBatch:],labels[:endOfBacth]))\n","\n","        return dataBatch, labelsBatch\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"02rH3mJ_QHiN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["if __name__ == \"__main__\":\n","\n","    # Parameters\n","    learning_rate = 0.001\n","    training_iters = 100000\n","    batch_size = 64\n","    display_step = 1\n","    train_size = 800\n","\n","    # Network Parameters\n","    # n_input = 599 * 128\n","    n_input = 599 * 128*2\n","    n_classes = 10\n","    dropout = 0.75  # Dropout, probability to keep units\n","\n","    # Load data\n","    #data = []\n","    #with open(\"data\", 'r') as f:\n","    #    content = f.read()\n","    #    data = pickle.loads(content)\n","    #data = np.asarray(data)\n","    #data = data\n","    #data = data.reshape((data.shape[0], n_input))\n","\n","    #labels = []\n","    #with open(\"labels\", 'r') as f:\n","    #    content = f.read()\n","    #    labels = pickle.loads(content)\n","\n","    # #Hack\n","    # data = np.random.random((1000, n_input))\n","    # labels = np.random.random((1000, 10))\n","\n","    # Shuffle data\n","    #permutation = np.random.permutation(len(data))\n","    #data = data[permutation]\n","    #labels = labels[permutation]\n","\n","    # Split Train/Test\n","    #trainData = data[:train_size]\n","    #trainLabels = labels[:train_size]\n","\n","    #testData = data[train_size:]\n","    #testLabels = labels[train_size:]\n","\n","\n","    # tf Graph input\n","    x = tf.placeholder(tf.float32, [None, n_input])\n","    y = tf.placeholder(tf.float32, [None, n_classes])\n","    keep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"djU4bQJ3QhPE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":768},"outputId":"301a37ac-8cb1-4b85-c41d-73840e23b366","executionInfo":{"status":"error","timestamp":1531881932374,"user_tz":240,"elapsed":566,"user":{"displayName":"Connor Waldman","photoUrl":"//lh6.googleusercontent.com/-pI-MZ7D4bAE/AAAAAAAAAAI/AAAAAAAAAAk/kF3XA8bGbL8/s50-c-k-no/photo.jpg","userId":"104438406364969524223"}}},"cell_type":"code","source":["# Create model\n","    def conv2d(sound, w, b):\n","        return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(sound, w, strides=[1, 1, 1, 1],\n","                                                      padding='SAME'), b))\n","\n","\n","    def max_pool(sound, k):\n","        return tf.nn.max_pool(sound, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n","\n","\n","    def conv_net(_X, _weights, _biases, _dropout):\n","        # Reshape input picture\n","        _X = tf.reshape(_X, shape=[-1, 599, 128, 2])\n","\n","        # Convolution Layer\n","        conv1 = conv2d(_X, _weights['wc1'], _biases['bc1'])\n","        # Max Pooling (down-sampling)\n","        conv1 = max_pool(conv1, k=4)\n","        # Apply Dropout\n","        conv1 = tf.nn.dropout(conv1, _dropout)\n","\n","        # Convolution Layer\n","        conv2 = conv2d(conv1, _weights['wc2'], _biases['bc2'])\n","        # Max Pooling (down-sampling)\n","        conv2 = max_pool(conv2, k=2)\n","        # Apply Dropout\n","        conv2 = tf.nn.dropout(conv2, _dropout)\n","\n","        # Convolution Layer\n","        conv3 = conv2d(conv2, _weights['wc3'], _biases['bc3'])\n","        # Max Pooling (down-sampling)\n","        conv3 = max_pool(conv3, k=2)\n","        # Apply Dropout\n","        conv3 = tf.nn.dropout(conv3, _dropout)\n","\n","        # Fully connected layer\n","        # Reshape conv3 output to fit dense layer input\n","        dense1 = tf.reshape(conv3, [-1, _weights['wd1'].get_shape().as_list()[0]])\n","        # Relu activation\n","        dense1 = tf.nn.relu(tf.add(tf.matmul(dense1, _weights['wd1']), _biases['bd1']))\n","        # Apply Dropout\n","        dense1 = tf.nn.dropout(dense1, _dropout)  # Apply Dropout\n","\n","        # Output, class prediction\n","        out = tf.add(tf.matmul(dense1, _weights['out']), _biases['out'])\n","        return out\n","\n","\n","    # Store layers weight & bias\n","    weights = {\n","        # 4x4 conv, 1 input, 149 outputs\n","        'wc1': tf.Variable(tf.random_normal([4, 4, 2, 149])),\n","        # 4x4 conv, 149 inputs, 73 outputs\n","        'wc2': tf.Variable(tf.random_normal([4, 4, 149, 73])),\n","        # 4x4 conv, 73 inputs, 35 outputs\n","        'wc3': tf.Variable(tf.random_normal([4, 4, 73, 35])),\n","        # fully connected, 38*8*35 inputs, 2^13 outputs\n","        'wd1': tf.Variable(tf.random_normal([38 * 8 * 35, 8192])),\n","        # 2^13 inputs, 13 outputs (class prediction)\n","        'out': tf.Variable(tf.random_normal([8192, n_classes]))\n","    }\n","\n","    biases = {\n","        'bc1': tf.Variable(tf.random_normal([149])+0.01),\n","        'bc2': tf.Variable(tf.random_normal([73])+0.01),\n","        'bc3': tf.Variable(tf.random_normal([35])+0.01),\n","        'bd1': tf.Variable(tf.random_normal([8192])+0.01),\n","        'out': tf.Variable(tf.random_normal([n_classes])+0.01)\n","    }\n","\n","    # Construct model\n","    pred = conv_net(x, weights, biases, keep_prob)\n","\n","    # Define loss and optimizer\n","    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n","    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","    # Evaluate model\n","    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n","    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n","    # Initializing the variables\n","    init = tf.initialize_all_variables()\n","\n","    # Add ops to save and restore all the variables.\n","    saver = tf.train.Saver()\n","\n","    # Launch the graph\n","    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n","        sess.run(init)\n","        step = 1\n","        # Keep training until reach max iterations\n","        while step * batch_size < training_iters:\n","            batch_xs, batch_ys = getBatch(trainData, trainLabels, batch_size, step)\n","            # Fit training using batch data\n","            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n","\n","            if step % display_step == 0:\n","                # Calculate batch accuracy\n","                acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n","                # Calculate batch loss\n","                loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n","                print(\"Iter \" + str(step * batch_size) + \", Minibatch Loss= \" + \\\n","                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n","\n","                save_path = saver.save(sess, \"model.ckpt\")\n","                print(\"Model saved in file: %s\" % save_path)\n","            step += 1\n","        print(\"Optimization Finished!\")\n","\n","        save_path = saver.save(sess, \"model.final\")\n","        print(\"Model saved in file: %s\" % save_path)\n","\n","        # Calculate accuracy\n","        print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: testData,\n","                                                                 y: testLabels,\n","                                                                 keep_prob: 1.}))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-10-cb40dc4d47ca>:74: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n","\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-10-cb40dc4d47ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Define loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m               instructions)\n\u001b[0;32m--> 250\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    252\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1959\u001b[0m   \"\"\"\n\u001b[1;32m   1960\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel, labels,\n\u001b[0;32m-> 1961\u001b[0;31m                     logits)\n\u001b[0m\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m   with ops.name_scope(name, \"softmax_cross_entropy_with_logits_sg\",\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[0;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[1;32m   1773\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[0;32m-> 1775\u001b[0;31m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[1;32m   1776\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"]}]}]}